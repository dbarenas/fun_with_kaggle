{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.preprocessing import Normalizer, normalize, scale, StandardScaler\n",
      "from sklearn.linear_model import SGDClassifier\n",
      "from sklearn.svm import LinearSVC, SVC\n",
      "from sklearn.cross_validation import KFold, cross_val_score\n",
      "from sklearn.pipeline import Pipeline, FeatureUnion\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "import evergreen\n",
      "reload(evergreen)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "<module 'evergreen' from 'evergreen.pyc'>"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## load data\n",
      "train_data = evergreen.read_data('data/train.tsv')\n",
      "test_data = evergreen.read_data('data/test.tsv')\n",
      "print train_data.shape, test_data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(7395, 18) (3171, 17)\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## encode discrete variables\n",
      "alchemy_descritizer = evergreen.AlchemyDiscretizer()\n",
      "train_data = alchemy_descritizer.fit_transform(train_data)\n",
      "test_data = alchemy_descritizer.transform(test_data)\n",
      "print train_data.shape, test_data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(7395, 30) (3171, 29)\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## extract target and nontext features\n",
      "train_nontext = evergreen.NontextFeatures().fit_transform(train_data)\n",
      "test_nontext = evergreen.NontextFeatures().fit_transform(test_data)\n",
      "train_target = evergreen.TargetLabel().fit_transform(train_data)\n",
      "print type(train_nontext), type(test_nontext), type(train_target)\n",
      "print train_nontext.shape, test_nontext.shape, train_target.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'numpy.ndarray'> <type 'numpy.ndarray'> <type 'numpy.ndarray'>\n",
        "(7395, 28) (3171, 28) (7395,)\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## cross validation\n",
      "n_samples = train_nontext.shape[0]\n",
      "cv = KFold(n_samples, n_folds=3, shuffle=True, random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## nontext feature by RandomTreesEmbedding\n",
      "from sklearn.ensemble import RandomTreesEmbedding\n",
      "rte = RandomTreesEmbedding(n_jobs=-1, )\n",
      "rte.fit(train_nontext)\n",
      "nontext_feats = rte.transform(train_nontext)\n",
      "sgd = SGDClassifier(loss = 'log', penalty='elasticnet', shuffle=True, )\n",
      "pipeline = Pipeline(steps=[\n",
      "    ('rte', RandomTreesEmbedding()),\n",
      "    ('sgd', SGDClassifier(loss = 'log', penalty='elasticnet', shuffle=True))\n",
      "])\n",
      "param_grid = {\n",
      "    'rte__n_estimators': [50, 100, 200, 300, 400, 500],\n",
      "    'rte__max_depth': [1, 2, 5, 10, 15],\n",
      "    'sgd__alpha': np.logspace(-6, -1, 6)\n",
      "}\n",
      "gs = GridSearchCV(pipeline, param_grid, scoring='roc_auc', n_jobs=-1, cv = cv, )\n",
      "gs.fit(train_nontext, train_target)\n",
      "print gs.best_params_\n",
      "print gs.best_score_\n",
      "cross_val_score(gs.best_estimator_, train_nontext, train_target, cv = cv, n_jobs = -1, scoring='roc_auc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'rte__n_estimators': 400, 'rte__max_depth': 2, 'sgd__alpha': 0.001}\n",
        "0.721288011301\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "array([ 0.70361098,  0.7100839 ,  0.69155446])"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## test nontext features on linear and nonlinear classifier - RMBR TO SCALE FEATURES\n",
      "sgd = SGDClassifier(loss = 'log', penalty='elasticnet', shuffle=True)\n",
      "print cross_val_score(sgd, scale(train_nontext), train_target, cv = cv, n_jobs=-1, scoring='roc_auc')\n",
      "\n",
      "svc = SVC(C = 10, gamma=0.01)\n",
      "print cross_val_score(svc, scale(train_nontext), train_target, cv = cv, n_jobs=-1, scoring='roc_auc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.59769186  0.67268379  0.62882745]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.73090792  0.74515095  0.73763232]\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## text data - BOW based on soft_thresholding\n",
      "train_text = evergreen.TextFeatures().fit_transform(train_data)\n",
      "test_text = evergreen.TextFeatures().fit_transform(test_data)\n",
      "\n",
      "bow = evergreen.SoftThresholdFeatures()\n",
      "train_bow = bow.fit_transform(train_text)\n",
      "print train_bow.shape\n",
      "test_bow = bow.transform(test_text)\n",
      "print test_bow.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "can't multiply sequence by non-int of type 'unicode'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-12-22e72956abd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevergreen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSoftThresholdFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mtrain_bow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/lima/workspace/fun_with_kaggle/kaggle-evergreen/evergreen.pyc\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenters_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoft_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcenters_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mWord2VecFeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransformerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/lima/workspace/fun_with_kaggle/kaggle-evergreen/utils.pyc\u001b[0m in \u001b[0;36msoft_threshold\u001b[0;34m(data, centers, thr, normalized)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mcoursety\u001b[0m \u001b[0mof\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mnbviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m4403811\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mMNIST\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0mnon\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20l\u001b[0m\u001b[0minear\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0mexpansion\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;32mfor\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0mclassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipynb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \t\"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Python/2.7/site-packages/sklearn/preprocessing/data.pyc\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy)\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnorms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m             \u001b[0mnorms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0mnorms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnorms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mnorms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'unicode'"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Library/Python/2.7/site-packages/sklearn/utils/validation.py:278: UserWarning: The normalize function assumes floating point values as input, got object\n",
        "  \"got %s\" % (estimator, X.dtype))\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## test nontext features on linear and nonlinear classifier - NO SCALING FOR SPASE FEATURES\n",
      "sgd = SGDClassifier(loss = 'log', penalty='elasticnet', shuffle=True)\n",
      "print cross_val_score(sgd, train_bow, train_target, cv = cv, n_jobs=-1, scoring='roc_auc')\n",
      "\n",
      "svc = SVC(C = 10, gamma=0.01)   \n",
      "print cross_val_score(svc, train_bow, train_target, cv = cv, n_jobs=-1, scoring='roc_auc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.85140218  0.85442325  0.8639967 ]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.85386552  0.86017513  0.86039051]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Combined Features, single Model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## PUTTING nontext and text-bow together\n",
      "nontext_feature_extractor = Pipeline(steps = [\n",
      "    ('alchemy_descritizer', evergreen.AlchemyDiscretizer()),\n",
      "    ('nontext', evergreen.NontextFeatures()),\n",
      "    ('scaler', StandardScaler())\n",
      "])\n",
      "bow_feature_extractor = Pipeline(steps = [\n",
      "    ('text', evergreen.TextFeatures()),\n",
      "    ('hasherizer', HashingVectorizer(encoding = 'iso-8859-1', \n",
      "\t\t\t\t\t\tstop_words='english', non_negative=True, \n",
      "\t\t\t\t\t\tn_features=100000)),\n",
      "    ('soft_thr', evergreen.SoftThresholdFeatures())\n",
      "])\n",
      "w2v_feature_extractor = Pipeline(steps = [\n",
      "    ('text', evergreen.TextFeatures()),\n",
      "    ('word2vec', evergreen.Word2VecFeatures(500, 'tmp/bigtext-classes500.txt'))\n",
      "])\n",
      "feature_extractor = FeatureUnion([\n",
      "        ('nontext', nontext_feature_extractor)\n",
      "        ,('text_soft_thr', bow_feature_extractor)\n",
      "        ,('text_word2vec', w2v_feature_extractor)\n",
      "])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_feat = feature_extractor.fit_transform(evergreen.read_data('data/train.tsv'))\n",
      "test_feat = feature_extractor.transform(evergreen.read_data('data/test.tsv'))\n",
      "train_target = evergreen.TargetLabel().fit_transform(evergreen.read_data('data/train.tsv'))\n",
      "print train_feat.shape, test_feat.shape, train_target.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(7395, 2028) (3171, 2028) (7395,)\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = KFold(train_feat.shape[0], n_folds=3, shuffle=True, random_state=0)\n",
      "svc = SVC(C = 10, gamma=0.01)\n",
      "param_grid = {\n",
      "    'C': [1, 10, 1e3, 1e4],\n",
      "    'gamma': [0, 0.1, 1e-2, 1e-3]\n",
      "}\n",
      "\n",
      "gs = GridSearchCV(svc, param_grid, scoring='roc_auc', cv = cv, n_jobs=-1, )\n",
      "%time gs.fit(train_feat, train_target)\n",
      "print gs.best_params_\n",
      "print gs.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 1min 25s, sys: 9.14 s, total: 1min 34s\n",
        "Wall time: 25min 20s\n",
        "{'C': 1, 'gamma': 0}\n",
        "0.847939780611\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = KFold(train_feat.shape[0], n_folds=3, shuffle=True, random_state=0)\n",
      "svc = SVC(C = 1, gamma=0.)\n",
      "print cross_val_score(svc, train_feat, train_target, cv = cv, n_jobs=-1, scoring='roc_auc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.83963522  0.85152621  0.85265791]\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## bigram and bag-of-word model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## PUTTING nontext and text-bow together\n",
      "nontext_feature_extractor = Pipeline(steps = [\n",
      "    ('alchemy_descritizer', evergreen.AlchemyDiscretizer()),\n",
      "    ('nontext', evergreen.NontextFeatures()),\n",
      "    ('scaler', StandardScaler())\n",
      "])\n",
      "bow_feature_extractor = Pipeline(steps = [\n",
      "    ('text', evergreen.TextFeatures()),\n",
      "    ('hasherizer', HashingVectorizer(encoding = 'iso-8859-1', ngram_range=(1, 2),\n",
      "\t\t\t\t\t\tstop_words='english', non_negative=True, \n",
      "\t\t\t\t\t\tn_features=100000)),\n",
      "    ('soft_thr', evergreen.SoftThresholdFeatures(n_features=2000))\n",
      "])\n",
      "w2v_feature_extractor = Pipeline(steps = [\n",
      "    ('text', evergreen.TextFeatures()),\n",
      "    ('word2vec', evergreen.Word2VecFeatures(500, 'tmp/bigtext-classes500.txt'))\n",
      "])\n",
      "bigram_feature_extractor = Pipeline(steps = [\n",
      "    ('text', evergreen.TextFeatures()),\n",
      "    ('tfidf', TfidfVectorizer(encoding='iso-8859-1', stop_words='english', \n",
      "                              ngram_range=(2, 2), max_df = 0.5, ))\n",
      "])\n",
      "feature_extractor = FeatureUnion([\n",
      "        ('nontext', nontext_feature_extractor)\n",
      "        ,('text_soft_thr', bow_feature_extractor)\n",
      "        ,('text_word2vec', w2v_feature_extractor)\n",
      "        #,('text_bigram', bigram_feature_extractor)\n",
      "])\n",
      "\n",
      "train_feat = feature_extractor.fit_transform(evergreen.read_data('data/train.tsv'))\n",
      "test_feat = feature_extractor.transform(evergreen.read_data('data/test.tsv'))\n",
      "train_target = evergreen.TargetLabel().fit_transform(evergreen.read_data('data/train.tsv'))\n",
      "print train_feat.shape, test_feat.shape, train_target.shape\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(7395, 2528) (3171, 2528) (7395,)\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = KFold(train_feat.shape[0], n_folds=3, shuffle=True, random_state=0)\n",
      "svc = SVC(C = 10, gamma=0.01)\n",
      "param_grid = {\n",
      "    'C': [1, 10, 1e3, 1e4],\n",
      "    'gamma': [0, 0.1, 1e-2, 1e-3]\n",
      "}\n",
      "\n",
      "gs = GridSearchCV(svc, param_grid, scoring='roc_auc', cv = cv, n_jobs=-1, )\n",
      "%time gs.fit(train_feat, train_target)\n",
      "print gs.best_params_\n",
      "print gs.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = KFold(train_feat.shape[0], n_folds=3, shuffle=True, random_state=0)\n",
      "sgd = SGDClassifier(loss = 'log', penalty='elasticnet', )\n",
      "param_grid = {\n",
      "    'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
      "    'l1_ratio': [0.1, 0.15, 0.3, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
      "}\n",
      "\n",
      "gs = GridSearchCV(sgd, param_grid, scoring='roc_auc', cv = cv, n_jobs=-1, )\n",
      "%time gs.fit(train_feat, train_target)\n",
      "print gs.best_params_\n",
      "print gs.best_score_"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CPU times: user 18.3 s, sys: 30.8 s, total: 49.1 s\n",
        "Wall time: 1min 15s\n",
        "{'alpha': 0.1, 'l1_ratio': 0.8}\n",
        "0.830463732819\n"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = KFold(train_feat.shape[0], n_folds=3, shuffle=True, random_state=0)\n",
      "svc = SVC(C = 1, gamma=0.)\n",
      "print cross_val_score(svc, train_feat, train_target, cv = cv, n_jobs=-1, scoring='roc_auc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cv = KFold(train_feat.shape[0], n_folds=3, shuffle=True, random_state=0)\n",
      "sgd = SGDClassifier(loss = 'log', penalty='elasticnet')\n",
      "print cross_val_score(sgd, train_feat, train_target, cv = cv, n_jobs=-1, scoring='roc_auc')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 0.77100043  0.73140048  0.74727255]\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Different features, combined model"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "train_data = evergreen.read_data('data/train.tsv')\n",
      "train_target = evergreen.TargetLabel().fit_transform(evergreen.read_data('data/train.tsv'))\n",
      "test_data = evergreen.read_data('data/test.tsv')\n",
      "n_samples = train_data.shape[0]\n",
      "alchemy_discritizer = evergreen.AlchemyDiscretizer()\n",
      "scaler = StandardScaler()\n",
      "\n",
      "nontext_preprocess_step = Pipeline(steps = [\n",
      "    ('alchemy_descritizer', alchemy_discritizer),\n",
      "    ('nontext', evergreen.NontextFeatures()),\n",
      "])\n",
      "text_preprocess_step = Pipeline(steps = [\n",
      "    ('text', evergreen.TextFeatures()),\n",
      "    ('hasherizer', HashingVectorizer(encoding = 'utf-8', \n",
      "\t\t\t\t\t\tstop_words='english', non_negative=True, \n",
      "\t\t\t\t\t\tn_features=100000))\n",
      "])\n",
      "train_nontext = nontext_preprocess_step.fit_transform(train_data)\n",
      "test_nontext = nontext_preprocess_step.fit_transform(test_data)\n",
      "train_text = text_preprocess_step.fit_transform(train_data)\n",
      "test_text = text_preprocess_step.fit_transform(test_data)\n",
      "\n",
      "\n",
      "nontext_model = Pipeline(steps = [\n",
      "    ('scaler', StandardScaler()),\n",
      "    ('svc', SVC(C = 10, gamma=0.1, probability=True))\n",
      "])\n",
      "\n",
      "text_model = Pipeline([\n",
      "    ('soft_thr', evergreen.SoftThresholdFeatures()),\n",
      "    ('svc', SVC(C = 10, gamma=0.1, probability=True))\n",
      "])\n",
      "\n",
      "## DONT USE n_jobs = -1\n",
      "#preprocess_step = FeatureUnion([\n",
      "#    ('nontext', nontext_preprocess_step),\n",
      "#    ('text', text_preprocess_step)\n",
      "#])\n",
      "\n",
      "cv = KFold(n_samples, n_folds=3, shuffle=True, random_state=0)\n",
      "for i, (c, v) in enumerate(cv):\n",
      "    print 'CV %i' % i\n",
      "    nontext_model.fit(train_nontext[c], train_target[c])\n",
      "    text_model.fit(train_text[c], train_target[c])\n",
      "    nontext_yhat = nontext_model.predict_proba(train_nontext[v])\n",
      "    text_yhat = text_model.predict_proba(train_text[v])\n",
      "    yhat = evergreen.combine_predictions([nontext_yhat, text_yhat])\n",
      "    print 'AUC: ', evergreen.calculate_auc(train_target[v], yhat)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "CV 0\n",
        "AUC: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.712503557162\n",
        "CV 1\n",
        "AUC: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.712191491297\n",
        "CV 2\n",
        "AUC: "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.703870117658\n"
       ]
      }
     ],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}