1. OUTPUT: (interested, not_interested), (0, 0) about 10%, (1, 1) - none (could be a hard constraint)

2. INPUT:(relevance extraction) - whether a user is interested in an event, depends on (roughly ranking)
whether_user_is_invited - in train/test
user_location
user_local
event_city_state_country_lat_long (if not missing)
event_topic_term_of_freq(common words)
user_birthyear
user_gender
user_friends_event_creator
user_friends_event_invited
time_of_event
time_user_see_event
user_jointime
user_attendee_intention (yes/maybe/invitited to/not going)

3. to explore
(*) common missing values - None
(*) parse date - use pd.csv and dateutil.parser
(*) all users in test are in users.csv - yes
(*) all users in test are in train.csv - NO (1357 not in train)
(*) all events in test are in events.csv - yes
(*) all events in test are in train.csv - NO (4572 not in train) - not really matter?
(*) read and efficiently use large events.csv - ?? iterator=true
(*) handle missing values
(*) correlation between each inputs and output

(*) preference representation - 4-value (not_interested 0, empty 1, interested 2)
3-value (not_interested 0, interested 1) TOO SPARSE FOR PEARSON, 2-value (interested) COULD TOO SPARSE FOR PEARSON
(*) similarity measure - pearson is less preferred bcs there aint a lot of rating values, so the variance of one users preference is easy to be zero, not to mention the overlapping of different users are sparse.
(*) for boolean preference, use TanimotoCoefficientSimilarity/LogLikelihoodSimilarity might give better result

4. to experiment with
(---) challenge is: 
(--1) some users and events have no overlapping - sparseness
(--2) some users or events have never been in training - anonymous/new users (cold start problem)

(1) As a recommendation system:

(*) data representation - boolean/three-valued/real-valued preference?
(**) how to capture ignorance/noise?
(**) be smart about the data - use events and creator as another train dataset, not it is sparse
(**) modeling "like" relation - is modeling "not like" really that useful?

(*) collaborative filtering / content based recommendation

(*) user/item based recommendation 
(**) number of users/events - computation time
(**) ratings say more about users or events? - bidirectional?

(*) recommendation similarity (and neigbhorhood for user-based):
(**) performance of different similarity measures
(**) how to customize them by domain-specific knowledge injection?
(**) neighborhood size k or threshold
(**) be careful with Tanimoto and log-likelihood similarity when used with certain data representation

(*) injection of domain-specific knowledge
(**) customized ItemSimilarity/UserSimilarity
(**) customized recommendation system with a rescorer

(*) recommendation methods

(*) combined models - 
(**) use some collaborative filtering to find a LARGE enough candidates of recommendation items
(**) use classification approach or other models to refine the recommendation list

(2) As a classification/regression and/or combined system:
(*) meaningful features to human beings - feature extraction makes sense
(*) just found the submission should be RANKING events in the given set in test. that makes more sense now to treat it as a CLASSIFICATION problem!

(*) the events are distinguishable enough, but the users are NOT
(*) ON the other hand, to RANK different events for the same user, we also 
need events distinguishable for different users
(*) ideally, users should be classified/clustered first, to form groups, based on their preference on events, and different classifiers are built for each user group, to classify their event interests.
(*) missing piece - users are not quite distinguishable as events - clusters them based on their friends! 
http://dev.bizo.com/2012/01/clustering-of-sparse-data-using-python.html
(*) for recommendation rank - too many rank 4 (no interest, no non-interest, no invitation) - can we further distinguish them?


(***) IN TRAIN.CSV, SAME USER_ID, EVENT_ID, DIFFERENT ROWS! e.g. uid, eid = 203456139	1462902079, usually they
have different NOTIFICATION TIME! so it seems to be fine...
